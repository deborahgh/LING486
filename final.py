# -*- coding: utf-8 -*-
"""Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T7C90ZVkcwyIUaYmkNbmcYl6cCf7atMB
"""

#Part 1: Cleaning the Text, Segmenting them and Tokenizing them
from google.colab import drive
drive.mount('/content/drive', force_remount=True)
import string                
import tensorflow as tf     
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer

rawText = open('/content/drive/MyDrive/LING486_Midterm/pride_and_prejudice_cleaned.txt', 'r')
text = str(rawText.read())
text = text.replace('_' , '')
text = text.replace('Mr. ' , 'Mr') 
text = text.replace('Mrs. ' , 'Mrs')
text = text.replace('\n' , ' ')
text = text.replace('      ' , '')
punct = string.punctuation
for p in punct:
  text = text.replace(p, ".")
phrases = []
phrases = text.split(".")
for phrase in phrases:
  if phrase == '  ':
    phrases.remove('  ')
  if phrase == '':
    phrases.remove('')
  if phrase == ' ':
    phrases.remove(' ')

tokenizer = Tokenizer(num_words = 20000)
tokenizer.fit_on_texts(phrases)
word_index = tokenizer.word_index
sequences = tokenizer.texts_to_sequences(phrases)
word_dict = []
for i in word_index:
  word_dict.append(i)

gather = []
from keras.utils import to_categorical #for OHE
from itertools import compress #for getting pairs
for test in sequences:
  res = list(zip(test, test[1:] + test[:1]))
  res = res[:-1]  #gets rid of last element b/c it's last word loop to first word (ie. 1st line: (4, 3))
  gather.append(res)

IN, OUT = [], []
for gat in gather:
  for g in gat:
    IN.append(g[0])
    OUT.append(g[1])

OHEencoded = to_categorical(IN)

from keras.layers import Input, Dense
from keras.models import Model
from keras.datasets import mnist
import numpy as np
import matplotlib.pyplot as plt

bottleneck = 9 

input = Input(shape=(6956,))
encoded = Dense(128, activation='relu')(input)
encoded = keras.layers.LeakyReLU()(encoded)
encoded = Dense(bottleneck, activation='relu')(input)
encoded = keras.layers.LeakyReLU()(encoded)
decoded = Dense(6956, activation='sigmoid')(encoded)

autoencoder = Model(input, decoded)

encoder = Model(input, encoded)
encoded_input = Input(shape=(bottleneck,))

decoder_layer = autoencoder.layers[-1]
decoder = Model(encoded_input, decoder_layer(encoded_input))

autoencoder.compile(optimizer='adam', loss="sparse_categorical_crossentropy", metrics=["accuracy"])

training_data, training_labels = [], []
test_data, test_labels = [], []
for i in range(len(OHEencoded)):
  if i % 4 == 1:
    test_data.append(OHEencoded[i])
    test_labels.append(OUT[i])
  else:
    training_data.append(OHEencoded[i])
    training_labels.append(OUT[i])

training_data = np.array(training_data)
training_labels = np.array(training_labels)
test_data = np.array(test_data)
test_labels = np.array(test_labels) 

autoencoder.fit(training_data, training_labels, epochs=50, batch_size=256, shuffle=True, validation_data=(test_data, test_labels), verbose=1)

import matplotlib.pyplot as plt
from numpy import argmax
from random import randrange

class_names = ['Determiner', 'Noun', 'Pronoun', 'Verb', 'Adverb', 'Adjective', 'Conjunction', 'Preposition', 'Injection']

encoded_result = encoder.predict(test_data)
decoded_result = decoder.predict(encoded_result)
prediction = encoded_result
#prediction = decoded_result

for i in range(30):
  rand = randrange(26607)
  inverted = argmax(test_data[rand]) 
  word = word_dict[inverted - 1] 
  print("Prediction: " + word + " " + str(inverted) + " " + str(prediction[rand]) + " " + class_names[(np.argmax(prediction[rand]))%9]) 
  plt.plot(prediction[rand]) 
  plt.title('POS Graph')
  plt.ylabel('probability') 
  plt.xlabel('POS')  
  name = class_names[(np.argmax(prediction[rand]))%9]
  plt.legend(name, loc='best')
  plt.show()